{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from utils.evaluation_framework import EvaluationFramework\n",
    "\n",
    "random.seed(42)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data\n",
    "df_real_data = pd.read_csv(\"./Data/Real/dataset.csv\")\n",
    "\n",
    "d_synthetic_data = {}\n",
    "d_synthetic_data[\"Method1\"] = pd.read_csv(f\"./Data/Synthetic/Synthetic_Method1.csv\")\n",
    "d_synthetic_data[\"Method2\"] = pd.read_csv(f\"./Data/Synthetic/Synthetic_Method2.csv\")\n",
    "d_synthetic_data[\"Method3\"] = pd.read_csv(f\"./Data/Synthetic/Synthetic_Method3.csv\")\n",
    "\n",
    "# Get a list with categorical features' names\n",
    "categorical_features = [\n",
    "    feature for feature in df_real_data.select_dtypes(include=\"object\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    # Real data\n",
    "    df_real_data[feature] = label_encoder.fit_transform(df_real_data[feature])\n",
    "    # Syntheric data\n",
    "    for dataset in d_synthetic_data:\n",
    "        d_synthetic_data[dataset][feature] = label_encoder.transform(\n",
    "            d_synthetic_data[dataset][feature]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = EvaluationFramework(df_real_data, d_synthetic_data, categorical_features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein/Cramers-v test\n",
      "--------------------------------------------------\n",
      "Method2 score: 18.0588\n",
      "Method1 score: 29.6471\n",
      "Method3 score: 30.2941\n",
      "\n",
      "\n",
      "Anomaly detection test\n",
      "--------------------------------------------------\n",
      "Method1 score: 0.0090\n",
      "Method2 score: 0.0060\n",
      "Method3 score: 0.0030\n",
      "\n",
      "\n",
      "Method: Method1\n",
      "> (Train) AUC score: 99.20\n",
      "> (Test) AUC score: 97.63\n",
      "Method: Method2\n",
      "> (Train) AUC score: 99.22\n",
      "> (Test) AUC score: 96.77\n",
      "Method: Method3\n",
      "> (Train) AUC score: 99.06\n",
      "> (Test) AUC score: 98.22\n",
      "\n",
      "\n",
      "Domain classifier test\n",
      "--------------------------------------------------\n",
      "Method1 score: 97.63\n",
      "Method2 score: 96.77\n",
      "Method3 score: 98.22\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wasserstein/Cramers-v test\n",
    "score_wasserstein_cramers_v = evaluation.wasserstein_cramers_v_test()\n",
    "\n",
    "# Novelty test\n",
    "score_novelty = evaluation.novelty_test()\n",
    "\n",
    "# Anomaly detection test\n",
    "score_anomaly = evaluation.anomaly_detection()\n",
    "\n",
    "# Domain classifier test\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "xgbc = RandomForestClassifier(n_estimators=50, max_depth=5)\n",
    "score_classification = evaluation.domain_classifier(model=xgbc, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] H0: {All methods exhibited similar results with no statistical differences}\n",
      "[INFO] FAR: 3.556 (p-value: 0.16901) - H0 is failed to be rejected)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>FAR</th>\n",
       "      <th>APV</th>\n",
       "      <th>Null hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Method2</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Method1</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Method3</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Methods       FAR       APV Null hypothesis\n",
       "0  Method2  2.333333         -               -\n",
       "1  Method1  6.333333  0.000127        Rejected\n",
       "2  Method3  6.333333  0.000127        Rejected"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistical analysis results\n",
    "Ranking = evaluation.get_synthesizers_ranking()\n",
    "display(Ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
